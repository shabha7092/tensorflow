{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_place_holders(n_x, n_y):\n",
    "    if n_x is None or n_y is None:\n",
    "        return None\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], \"X\")\n",
    "    y = tf.placeholder(tf.float32, [n_y, None], \"y\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nx, layers):\n",
    "    if nx is None or layers is None or len(layers) == 0:\n",
    "        return None\n",
    "    parameters = {}\n",
    "    for i, k in enumerate(layers):\n",
    "        parameters['W' + str(i+1)] = tf.get_variable('W' + str(i+1), [layers[i], nx], dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        parameters['b' + str(i+1)] = tf.get_variable('b' + str(i+1), [layers[i], 1],  dtype=tf.float32, initializer = tf.zeros_initializer())\n",
    "        nx = layers[i]\n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    if X is None or parameters is None or len(parameters) == 0:\n",
    "        return None\n",
    "    A = X\n",
    "    for i in range(int(len(parameters)/2)):\n",
    "        if i == (len(parameters)/2) - 1:\n",
    "            Z = tf.add(tf.matmul(parameters['W' + str(i+1)], A), parameters['b' + str(i+1)])\n",
    "            return Z\n",
    "        Z = tf.add(tf.matmul(parameters['W' + str(i+1)], A), parameters['b' + str(i+1)])\n",
    "        A = tf.nn.relu(Z) \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    if Z is None or Y is None:\n",
    "        return None\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)   \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed=0):\n",
    "    m = X.shape[1]                 \n",
    "    mini_batches = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, layers, learning_rate = 0.0001, num_epochs = 100, minibatch_size = 32, print_cost = True):\n",
    "    if X_train is None or Y_train is None or X_test is None or Y_test is None or layers is None or len(layers) == 0:\n",
    "        return None\n",
    "    ops.reset_default_graph()\n",
    "    (n_x, m) = X_train.shape\n",
    "    (n_y, m) = Y_train.shape\n",
    "    tf.set_random_seed(1) \n",
    "    seed = 3 \n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_place_holders(n_x, n_y)\n",
    "    parameters = initialize_parameters(n_x, layers)\n",
    "    Z = forward_propogation(X, parameters)\n",
    "    cost = compute_cost(Z, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    num_minibatches = int(m / minibatch_size)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.0\n",
    "            seed = seed+1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "      # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[227 220 214]\n",
      "   [227 221 215]\n",
      "   [227 222 215]\n",
      "   ...\n",
      "   [232 230 224]\n",
      "   [231 229 222]\n",
      "   [230 229 221]]\n",
      "\n",
      "  [[227 221 214]\n",
      "   [227 221 215]\n",
      "   [228 221 215]\n",
      "   ...\n",
      "   [232 230 224]\n",
      "   [231 229 222]\n",
      "   [231 229 221]]\n",
      "\n",
      "  [[227 221 214]\n",
      "   [227 221 214]\n",
      "   [227 221 215]\n",
      "   ...\n",
      "   [232 230 224]\n",
      "   [231 229 223]\n",
      "   [230 229 221]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[119  81  51]\n",
      "   [124  85  55]\n",
      "   [127  87  58]\n",
      "   ...\n",
      "   [210 211 211]\n",
      "   [211 212 210]\n",
      "   [210 211 210]]\n",
      "\n",
      "  [[119  79  51]\n",
      "   [124  84  55]\n",
      "   [126  85  56]\n",
      "   ...\n",
      "   [210 211 210]\n",
      "   [210 211 210]\n",
      "   [209 210 209]]\n",
      "\n",
      "  [[119  81  51]\n",
      "   [123  83  55]\n",
      "   [122  82  54]\n",
      "   ...\n",
      "   [209 210 210]\n",
      "   [209 210 209]\n",
      "   [208 209 209]]]\n",
      "\n",
      "\n",
      " [[[238 232 223]\n",
      "   [238 232 223]\n",
      "   [238 232 223]\n",
      "   ...\n",
      "   [222 216 209]\n",
      "   [221 216 207]\n",
      "   [221 216 206]]\n",
      "\n",
      "  [[237 232 223]\n",
      "   [238 232 223]\n",
      "   [238 232 223]\n",
      "   ...\n",
      "   [222 216 209]\n",
      "   [222 216 208]\n",
      "   [223 217 207]]\n",
      "\n",
      "  [[236 232 222]\n",
      "   [237 232 223]\n",
      "   [238 232 223]\n",
      "   ...\n",
      "   [222 216 209]\n",
      "   [222 216 208]\n",
      "   [221 216 207]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[218 212 204]\n",
      "   [217 212 204]\n",
      "   [217 211 205]\n",
      "   ...\n",
      "   [214 203 194]\n",
      "   [214 203 195]\n",
      "   [214 204 194]]\n",
      "\n",
      "  [[217 211 203]\n",
      "   [217 211 203]\n",
      "   [216 210 203]\n",
      "   ...\n",
      "   [214 203 194]\n",
      "   [215 203 194]\n",
      "   [215 204 193]]\n",
      "\n",
      "  [[216 210 202]\n",
      "   [216 210 203]\n",
      "   [215 209 203]\n",
      "   ...\n",
      "   [214 203 194]\n",
      "   [215 203 194]\n",
      "   [215 204 192]]]\n",
      "\n",
      "\n",
      " [[[228 220 208]\n",
      "   [228 220 208]\n",
      "   [227 219 208]\n",
      "   ...\n",
      "   [231 228 221]\n",
      "   [232 228 221]\n",
      "   [231 227 220]]\n",
      "\n",
      "  [[228 219 208]\n",
      "   [228 219 208]\n",
      "   [227 219 207]\n",
      "   ...\n",
      "   [231 227 221]\n",
      "   [231 227 221]\n",
      "   [231 227 220]]\n",
      "\n",
      "  [[227 219 208]\n",
      "   [227 219 208]\n",
      "   [227 219 208]\n",
      "   ...\n",
      "   [231 227 222]\n",
      "   [231 227 222]\n",
      "   [231 227 220]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[214 200 185]\n",
      "   [214 199 183]\n",
      "   [184 167 150]\n",
      "   ...\n",
      "   [209 205 201]\n",
      "   [212 207 204]\n",
      "   [212 208 204]]\n",
      "\n",
      "  [[213 198 183]\n",
      "   [201 185 169]\n",
      "   [139 115  95]\n",
      "   ...\n",
      "   [209 204 201]\n",
      "   [210 206 202]\n",
      "   [212 207 203]]\n",
      "\n",
      "  [[209 194 178]\n",
      "   [162 142 124]\n",
      "   [122  88  64]\n",
      "   ...\n",
      "   [208 204 200]\n",
      "   [210 206 201]\n",
      "   [211 207 202]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[235 226 216]\n",
      "   [235 227 216]\n",
      "   [234 227 216]\n",
      "   ...\n",
      "   [228 222 213]\n",
      "   [228 222 213]\n",
      "   [228 223 213]]\n",
      "\n",
      "  [[235 226 216]\n",
      "   [234 227 216]\n",
      "   [234 227 216]\n",
      "   ...\n",
      "   [228 222 214]\n",
      "   [228 222 214]\n",
      "   [228 222 214]]\n",
      "\n",
      "  [[234 226 217]\n",
      "   [234 226 216]\n",
      "   [234 226 216]\n",
      "   ...\n",
      "   [228 222 214]\n",
      "   [228 223 214]\n",
      "   [228 223 214]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[209 197 185]\n",
      "   [209 198 185]\n",
      "   [210 199 186]\n",
      "   ...\n",
      "   [201 194 187]\n",
      "   [202 195 187]\n",
      "   [202 195 187]]\n",
      "\n",
      "  [[208 197 183]\n",
      "   [209 198 184]\n",
      "   [210 198 185]\n",
      "   ...\n",
      "   [200 192 184]\n",
      "   [201 193 185]\n",
      "   [201 193 184]]\n",
      "\n",
      "  [[207 196 182]\n",
      "   [209 197 184]\n",
      "   [210 198 185]\n",
      "   ...\n",
      "   [199 191 182]\n",
      "   [200 191 183]\n",
      "   [200 192 182]]]\n",
      "\n",
      "\n",
      " [[[233 226 217]\n",
      "   [233 227 218]\n",
      "   [232 228 218]\n",
      "   ...\n",
      "   [225 221 211]\n",
      "   [225 220 210]\n",
      "   [224 219 209]]\n",
      "\n",
      "  [[232 226 217]\n",
      "   [232 227 218]\n",
      "   [233 228 219]\n",
      "   ...\n",
      "   [224 220 210]\n",
      "   [224 220 210]\n",
      "   [223 219 208]]\n",
      "\n",
      "  [[232 227 217]\n",
      "   [231 227 217]\n",
      "   [231 227 218]\n",
      "   ...\n",
      "   [224 221 211]\n",
      "   [224 220 210]\n",
      "   [222 219 208]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[192 184 173]\n",
      "   [193 185 173]\n",
      "   [192 185 173]\n",
      "   ...\n",
      "   [208 205 203]\n",
      "   [208 204 202]\n",
      "   [207 204 201]]\n",
      "\n",
      "  [[191 184 173]\n",
      "   [191 184 173]\n",
      "   [191 184 173]\n",
      "   ...\n",
      "   [207 204 202]\n",
      "   [206 204 201]\n",
      "   [206 203 200]]\n",
      "\n",
      "  [[190 183 171]\n",
      "   [191 183 172]\n",
      "   [190 183 172]\n",
      "   ...\n",
      "   [206 202 200]\n",
      "   [205 202 199]\n",
      "   [204 201 198]]]\n",
      "\n",
      "\n",
      " [[[230 220 209]\n",
      "   [230 221 210]\n",
      "   [230 221 210]\n",
      "   ...\n",
      "   [232 225 215]\n",
      "   [232 225 215]\n",
      "   [231 224 214]]\n",
      "\n",
      "  [[230 221 209]\n",
      "   [230 221 210]\n",
      "   [230 221 210]\n",
      "   ...\n",
      "   [231 224 214]\n",
      "   [231 225 215]\n",
      "   [231 224 214]]\n",
      "\n",
      "  [[229 220 209]\n",
      "   [229 221 210]\n",
      "   [229 221 210]\n",
      "   ...\n",
      "   [231 225 215]\n",
      "   [230 225 215]\n",
      "   [229 224 213]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[216 207 200]\n",
      "   [217 207 200]\n",
      "   [218 208 200]\n",
      "   ...\n",
      "   [202 198 198]\n",
      "   [202 199 198]\n",
      "   [203 200 197]]\n",
      "\n",
      "  [[215 206 199]\n",
      "   [217 207 200]\n",
      "   [218 208 200]\n",
      "   ...\n",
      "   [201 199 198]\n",
      "   [202 199 197]\n",
      "   [202 199 197]]\n",
      "\n",
      "  [[214 205 198]\n",
      "   [216 206 199]\n",
      "   [217 207 199]\n",
      "   ...\n",
      "   [201 198 197]\n",
      "   [202 199 198]\n",
      "   [202 199 197]]]]\n",
      "Cost after epoch 0: 1.855702\n",
      "Parameters have been trained!\n",
      "Train Accuracy: 0.6564815\n",
      "Test Accuracy: 0.6333333\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "print(X_train_orig)\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "layers =[25, 12, 6]\n",
    "parameters = model(X_train, Y_train, X_test, Y_test, layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
