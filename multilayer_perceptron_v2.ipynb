{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, glob\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_data(input):\n",
    "#     if input is None:\n",
    "#         return None\n",
    "#     splits = tf.string_split([input], delimiter='#')\n",
    "#     user_id = splits.values[0]\n",
    "#     label_str = splits.values[1]\n",
    "#     label_value = tf.string_to_number(label_str, tf.float32)\n",
    "#     label = tf.clip_by_value(label_value, 0.0, 1.0, name=None)\n",
    "#     activity_indices = tf.string_split([splits.values[2]], delimiter=',')\n",
    "#     dense_activity_indices = tf.sparse_to_dense(\n",
    "#         sparse_indices=activity_indices.indices,\n",
    "#         output_shape=activity_indices.dense_shape,\n",
    "#         sparse_values=activity_indices.values,\n",
    "#         default_value='0',\n",
    "#         validate_indices = True,\n",
    "#         name = None\n",
    "#     )\n",
    "#     activities = tf.string_to_number(dense_activity_indices, out_type=tf.int32)\n",
    "#     return (user_id, label, activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data(data_path):\n",
    "#     if data_path is None:\n",
    "#         return None\n",
    "#     file_pattern = os.path.join(data_path, 'part-*')\n",
    "#     files = tf.gfile.Glob(file_pattern)\n",
    "#     dataset = tf.data.TextLineDataset(files)\n",
    "#     return dataset.map(parse_data).batch(50).shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = read_data('datasets/rnn').make_one_shot_iterator().get_next()\n",
    "\n",
    "# iter = read_data('datasets/rnn').make_one_shot_iterator()\n",
    "# print(iter.get_next())\n",
    "# user_id, label, activities = iter.get_next()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     (user_id, label, activities) = sess.run([user_id, label, activities])\n",
    "# print(user_id)\n",
    "# print(label)\n",
    "# print(activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data(file_path):\n",
    "#     if file_path is None:\n",
    "#         return None\n",
    "#     data = pd.read_csv(file_path,  sep='#|,', header=None, engine='python')\n",
    "#     msk = np.random.rand(len(data)) < 0.8\n",
    "#     train_data = data[msk]\n",
    "#     test_data = data[~msk]\n",
    "#     user_ids_train = np.array([train_data.iloc[:, 0].values])\n",
    "#     labels_train = np.array([train_data.iloc[:, 1].values])\n",
    "#     features_train = train_data.iloc[:,2:].T.values\n",
    "#     user_ids_test = np.array([test_data.iloc[:, 0].values])\n",
    "#     labels_test = np.array([test_data.iloc[:, 1].values])\n",
    "#     features_test = test_data.iloc[:,2:].T.values\n",
    "#     return user_ids_train, features_train, labels_train, user_ids_test, features_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    if data_path is None:\n",
    "        return None\n",
    "    data_files = glob.glob(os.path.join(data_path, 'part-*'))\n",
    "    data = pd.concat(map(lambda file: pd.read_csv(file, sep='#|,', header=None, engine='python'), data_files), axis = 0, ignore_index = True)\n",
    "    train_data, validate_data, test_data = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "    \n",
    "    user_ids_train = np.array([train_data.iloc[:, 0].values])\n",
    "    labels_train = np.array([train_data.iloc[:, 1].values])\n",
    "    features_train = train_data.iloc[:,2:].T.values\n",
    "      \n",
    "    user_ids_validate = np.array([validate_data.iloc[:, 0].values])\n",
    "    labels_validate = np.array([validate_data.iloc[:, 1].values])\n",
    "    features_validate = validate_data.iloc[:,2:].T.values\n",
    "    \n",
    "    user_ids_test = np.array([test_data.iloc[:, 0].values])\n",
    "    labels_test = np.array([test_data.iloc[:, 1].values])\n",
    "    features_test = test_data.iloc[:,2:].T.values\n",
    "    \n",
    "    return user_ids_train, features_train, labels_train, user_ids_validate, features_validate, labels_validate, user_ids_test, features_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_place_holders(n_x, n_y):\n",
    "    if n_x is None or n_y is None:\n",
    "        return None\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], \"X\")\n",
    "    y = tf.placeholder(tf.float32, [n_y, None], \"y\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nx, layers):\n",
    "    if nx is None or layers is None or len(layers) == 0:\n",
    "        return None\n",
    "    parameters = {}\n",
    "    for i, k in enumerate(layers):\n",
    "        parameters['W' + str(i+1)] = tf.get_variable('W' + str(i+1), [layers[i], nx], dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        parameters['b' + str(i+1)] = tf.get_variable('b' + str(i+1), [layers[i], 1],  dtype=tf.float32, initializer = tf.zeros_initializer())\n",
    "        nx = layers[i]\n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    if X is None or parameters is None or len(parameters) == 0:\n",
    "        return None\n",
    "    A = X\n",
    "    for i in range(int(len(parameters)/2)):\n",
    "        if i == (len(parameters)/2) - 1:\n",
    "            Z = tf.add(tf.matmul(parameters['W' + str(i+1)], A), parameters['b' + str(i+1)])\n",
    "            return Z\n",
    "        Z = tf.add(tf.matmul(parameters['W' + str(i+1)], A), parameters['b' + str(i+1)])\n",
    "        A = tf.nn.relu(Z) \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y, beta=0.001):\n",
    "    if Z is None or Y is None:\n",
    "        return None\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)    \n",
    "    L2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'W' in v.name]) * beta\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels) + L2)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed=0):\n",
    "    m = X.shape[1]                 \n",
    "    mini_batches = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_validate, Y_validate, X_test, Y_test, layers, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    if X_train is None or Y_train is None or X_validate is None or Y_validate is None or X_test is None or Y_test is None or layers is None or len(layers) == 0:\n",
    "        return None\n",
    "    ops.reset_default_graph()\n",
    "    (n_x, m) = X_train.shape\n",
    "    (n_y, m) = Y_train.shape\n",
    "    tf.set_random_seed(1) \n",
    "    seed = 3 \n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_place_holders(n_x, n_y)\n",
    "    parameters = initialize_parameters(n_x, layers)\n",
    "    Z = forward_propogation(X, parameters)\n",
    "    cost = compute_cost(Z, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    num_minibatches = int(m / minibatch_size)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.0\n",
    "            seed = seed+1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "      # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        #correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "        correct_prediction = tf.equal(tf.round(tf.sigmoid(Z)), Y)\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Validation Accuracy:\", accuracy.eval({X: X_validate, Y: Y_validate}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 35741)\n",
      "(1, 35741)\n",
      "[[ 54   5 123 ... 104 147   4]\n",
      " [ 20 263  99 ...  32 251   1]\n",
      " [132 175 141 ...  30 240  54]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n",
      "[[1 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# working model\n",
    "# user_id_train, X_train, Y_train, user_id_validate, X_validate, Y_validate, user_id_test, X_test, Y_test = read_data('datasets/rnn/566_3_100_v2')\n",
    "# layers =[X_train.shape[0], 12, 1]\n",
    "\n",
    "# print(user_id_train.shape)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# print(user_id_test.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)\n",
    "\n",
    "#parameters = model(X_train, Y_train, X_validate, Y_validate, X_test, Y_test, layers)\n",
    "\n",
    "\n",
    "# (1, 35989)\n",
    "# (100, 35989)\n",
    "# (1, 35989)\n",
    "# (1, 8990)\n",
    "# (100, 8990)\n",
    "# (1, 8990)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
